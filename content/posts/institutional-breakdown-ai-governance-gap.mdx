---
title: "Institutional Breakdown and the AI Governance Gap"
date: "2025-02-28"
excerpt: "Post-conflict institutional failure offers a sobering lens for understanding the structural vulnerabilities of emerging AI oversight bodies."
tags:
  - AI Governance
  - Policy
---

There is a pattern in post-conflict and post-crisis environments that should concern anyone thinking seriously about AI governance: institutions designed to provide oversight, accountability, and coordination tend to fail not because of a single catastrophic event, but through a gradual erosion of capacity, legitimacy, and structural coherence. The failure is usually visible in retrospect but difficult to diagnose in real time.

## The Anatomy of Institutional Failure

My graduate research at King's College London examined the structural conditions under which governance institutions collapse — particularly in fragile states and post-conflict settings. Several recurring dynamics emerged:

- **Mandate creep without capacity growth.** Institutions are asked to do more with the same or fewer resources. Their formal responsibilities expand while their actual capability to fulfill those responsibilities stagnates or declines.

- **Legitimacy deficits.** When the governed population (or in the AI context, the regulated industry and affected public) does not view an institution as legitimate, compliance becomes performative rather than substantive. The institution exists on paper but lacks real authority.

- **Coordination collapse.** In complex governance environments with multiple overlapping bodies, the absence of clear jurisdictional boundaries and communication protocols leads to gaps, redundancies, and eventually institutional paralysis.

- **Capture and co-optation.** Institutions meant to serve the public interest become increasingly responsive to the interests of the actors they are supposed to regulate. This dynamic is well-documented in post-conflict governance; it is equally relevant to AI oversight.

## The AI Governance Parallel

The emerging landscape of AI governance institutions — national AI safety institutes, international coordination bodies, industry self-regulatory frameworks — already exhibits early signs of several of these failure modes.

We see **mandate creep** in organizations asked to simultaneously promote AI innovation and ensure AI safety, with budgets and staffing that are inadequate for either task alone. We see **legitimacy questions** in governance bodies that lack meaningful representation from affected communities and the Global South. We see **coordination gaps** between national regulators operating with different frameworks, timelines, and political incentives.

The question is not whether these dynamics will manifest. They already have. The question is whether we can design AI governance institutions with sufficient structural resilience to resist the patterns of breakdown that have undermined oversight bodies in other domains.

## Designing for Resilience

Drawing on the institutional failure literature and my own operational experience, I would suggest several design principles for AI governance bodies:

**Narrow, clearly defined mandates** are more robust than expansive ones. An institution that tries to be everything to everyone will fail at all of it. Better to have multiple focused bodies with clear coordination mechanisms than a single body with an impossibly broad remit.

**Built-in review and adaptation mechanisms** are essential. Institutions need structural capacity to assess their own effectiveness and adapt — not through occasional external reviews, but through continuous internal feedback loops.

**Diversity of input, not just diversity of representation.** It is not enough to have diverse membership if the institution's decision-making processes systematically filter out dissenting or unconventional perspectives. Structural provisions for adversarial review, red-teaming of policy positions, and formal channels for external challenge are more important than demographic composition alone.

**Financial and operational independence** from the entities being regulated. This is basic governance hygiene, but it is already being compromised in some AI governance contexts where industry funding constitutes a significant portion of regulatory body budgets.

---

*This line of research draws on my graduate work in intelligence and international security and my operational experience with institutional coordination in crisis settings. I am particularly interested in how the institutional design literature can inform the current moment in AI governance. Reach out if this is an area you're working in.*
